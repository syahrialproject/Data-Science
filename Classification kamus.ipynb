{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification\n",
    "\n",
    "#Train test Spit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#K-Nearest Neighbor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "#Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(gamma='scale')\n",
    "\n",
    "#Decission Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dt = DecisionTreeClassifier()\n",
    "\n",
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "\n",
    "#XGBoost\n",
    "import xgboost as xg\n",
    "model_xgb = xg.XGBClassifier(random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "#Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)\n",
    "\n",
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "for i in Categorical:\n",
    "    df[i] = LE.fit_transform(df[i])\n",
    "    \n",
    "#One hot encode\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "# logarithmic all data numeric\n",
    "for i in numerical:\n",
    "    df[i] = np.log(df[i]+1)\n",
    "    \n",
    "\n",
    "#PLOT Correlation\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(corr, annot=True, annot_kws={'size':9})\n",
    "plt.title('Correlation Clean Data')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "for col in feat_corr['mytsel_lapse'].index[1:]:\n",
    "    # Pick desired cutoff for dropping features. In absolute-value terms.\n",
    "    if np.absolute(feat_corr.loc['mytsel_lapse',col]) < 0.1:\n",
    "        # Drop the feature if correlation is below cutoff\n",
    "        data.drop(columns=col, inplace=True)\n",
    "print('-'*40)\n",
    "print('\\nData shape after feature selection:', data.shape)\n",
    "print('\\nCounts of lapse VS non-lapse in new data:')\n",
    "print(data['mytsel_lapse'].value_counts())\n",
    "print('-'*40)\n",
    "\n",
    "#SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "sm = SMOTETomek(random_state=42, sampling_strategy='minority')\n",
    "X_train_upsamp, Y_train_upsamp = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "#Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "\n",
    "#GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},\n",
    "              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=278,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    colsample_bytree=0.20651439788177967,\n",
    "    scale_pos_weight=2.35,\n",
    "    reg_alpha=3.6754725664416057,\n",
    "    reg_lambda=6.401966398857545,\n",
    "    gamma=0.2160029174594188,\n",
    "    learning_rate=0.1885401400753244\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Outlier\n",
    "# Removing Outliers from high-correlation features\n",
    "new_corr = data_reeng.corr()\n",
    "cols = new_corr['mytsel_lapse'].index[1:]\n",
    "\n",
    "# For each feature correlated with Class...\n",
    "for col in cols:\n",
    "    # If absolute correlation value is more than X percent...\n",
    "    correlation = new_corr.loc['mytsel_lapse',col]\n",
    "    if np.absolute(correlation) > 0.1:\n",
    "        \n",
    "        # Separate the classes of the high-correlation column\n",
    "        nonlapse = no_outliers.loc[no_outliers['mytsel_lapse']==0,col]\n",
    "        lapse = no_outliers.loc[no_outliers['mytsel_lapse']==1,col]\n",
    "\n",
    "        # Identify the 25th and 75th quartiles\n",
    "        all_values = no_outliers.loc[:,col]\n",
    "        q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n",
    "        # Get the inter quartile range\n",
    "        iqr = q75 - q25\n",
    "        # Smaller cutoffs will remove more outliers\n",
    "        cutoff = iqr * 1.5\n",
    "        # Set the bounds of the desired portion to keep\n",
    "        lower, upper = q25 - cutoff, q75 + cutoff\n",
    "        \n",
    "        # If positively correlated...\n",
    "        # Drop nonlapse above upper bound, and lapse below lower bound\n",
    "        if correlation > 0: \n",
    "            no_outliers.drop(index=nonlapse[nonlapse>upper].index,inplace=True)\n",
    "            no_outliers.drop(index=lapse[lapse<lower].index,inplace=True)\n",
    "        \n",
    "        # If negatively correlated...\n",
    "        # Drop nonlapse below lower bound, and lapse above upper bound\n",
    "        elif correlation < 0: \n",
    "            no_outliers.drop(index=nonlapse[nonlapse<lower].index,inplace=True)\n",
    "            no_outliers.drop(index=lapse[lapse>upper].index,inplace=True)\n",
    "        \n",
    "print('\\nData shape before removing outliers:', data_reeng.shape)\n",
    "print('\\nCounts of lapse VS non-lapse in previous data:')\n",
    "print(data_reeng['mytsel_lapse'].value_counts())\n",
    "print('-'*40)\n",
    "print('-'*40)\n",
    "print('\\nData shape after removing outliers:', no_outliers.shape)\n",
    "print('\\nCounts of lapse VS non-lapse in new data:')\n",
    "print(no_outliers['mytsel_lapse'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
